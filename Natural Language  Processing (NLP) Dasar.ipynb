{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28784704",
   "metadata": {},
   "source": [
    "# Pembuka \n",
    "\n",
    "Bismillahirrahmanirrahim\n",
    "\n",
    "Assalamu'alaikum warahmatullahi wabarakatuh\n",
    "\n",
    "Puji syukur kehadirat Allah Subhana Wata'ala atas limpahan Rahmat dan Hidayah-Nya kepada kita semua. Sholawat serta salam senantiasa tercurah limpahkan kepada baginda Muhammad Rasulullah Salallahualaihiwassalam.\n",
    "\n",
    "Halo para **Pejuang Data**. Selamat berjumpa di pertemuan pertama Program Training **Algoritma Machine Learning** Kelas Mahir. \n",
    "\n",
    "Pada pertemuan ini kamu akan belajar\n",
    "\n",
    "1.   Apa itu NLP\n",
    "2.   Scrapping Data\n",
    "     *  Case Folding dan Data Cleaning\n",
    "     *  Lemmatisasi\n",
    "     *  Stemming\n",
    "     *  Slang Word\n",
    "     *  Stop Word\n",
    "     *  Unwanted Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dc9198",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86f9187b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b54fa8",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "Natural Language Processing (NLP) merupakan salah satu cabang ilmu AI yang berfokus pada pengolahan bahasa natural. Bahasa natural adalah bahasa yang secara umum digunakan oleh manusia dalam berkomunikasi satu sama lain. Bahasan yang diterima oleh komputer butuh untuk diproses dan dipahami terlebih dahulu supaya maksud dari user bisa dipahami dengan baik oleh komputer.\n",
    "\n",
    "Ada berbagai alasan terapan aplikasi dari NLP. Diantaranya adalah Chatbot (aplikasi yang membuat user bisa seolah-olah melakukan komunikasi dengan komputer). Stemming atau Lemmatization (Pemotongan kata dalam bahasa tertentu menjadi bentuk dasar pengenalan fungsi setiap kata dalam kalimat). Summarization (ringkasan komputer mampu memahami instruksi bahasa yang diinputkan oleh user.\n",
    "\n",
    "Pustejovsky dan Stubbs (2012) menjelaskan bahwa ada beberapa area utama penelitian pada field NLP diantaranya:\n",
    "\n",
    "1.   **Question Answering System (QAS)**. Kemampuan komputer untuk menjawab pertanyaan yang diberikan oleh user. Daripada memasukkan keyword ke dalam browser pencarian, dengan QAS, user bisa langsung bertanya dalam bahasa natural yang digunakannya, baik itu Inggris, Mandarin, ataupun Indonesia.\n",
    "2.   **Summarization.** Pembuatan ringkasan dari sekumpulan konten dokumen atau email. Dengan menggunakan aplikasi ini, user bisa dibantu untuk mengkonversikan dokumen teks yang besar ke dalam bentuk slide presentasi. Machine Translation produk yang dihasilkan adalah aplikasi yang dapat memahami bahasa manusia dan menterjemahkannya ke dalam bahasa lain, termasuk didalamnya adalah Google Translate yang apabila dicermati semakin membaik dalam penterjemahan bahasa. Contoh lain lagi adalah BabelFish yang menterjemahkan bahasa pada real time.\n",
    "3.   **Speech Recognition.** Field ini merupakan cabang ilmu NLP yang cukup sulit. Proses pembangunan model untuk digunakan telpon/komputer dalam mengenali bahasa yang diucapkan sudah banyak dikerjakan. Bahasa yang sering digunakan adalah berupa pertanyaan atau perintah.\n",
    "4.   **Document Classification.** Sedangkan aplikasi ini adalah area penelitian NLP yang paling sukses. Pekerjaan yang dilakukan aplikasi ini adalah menentukan dimana tempat terbaik dokumen yang baru diinputkan ke dalam sistem. Hal ini sangat berguna pada aplikasi spam filtering, news article classification, dan movie review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95712c",
   "metadata": {},
   "source": [
    "# Scrapping Data Text\n",
    "\n",
    "Sebelum melakukan penerapan dan berbagai penelitian, mengumpulkan data teks sebagai bahan dasar dari bidang ini merupakan hal yang sangat penting. Proses ini biasa disebut dengan scrapping data. Aktivitas scapping data bisa dilakukan melalui berbagai platfrom. Mulai langsung pada halaman web tertentu, melalui API seperti twitter, atau melalui tools yang sudah disediakan bisa free atau berbayar. Untuk mulai belajar NLP, kita akan menggunakan tools. Tools/Library *google_play_scrapper* adalah library yang dapat digunakan untuk mengambil review dari google apps. Pertama kita perlu melakukan instalasi sebagai berikut."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b48a38",
   "metadata": {},
   "source": [
    "## Instalasi google play scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87972a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google_play_scraper in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install google_play_scraper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d31f31",
   "metadata": {},
   "source": [
    "**Import Library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "046333c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google_play_scraper import Sort, reviews       # Library untuk scrapping data\n",
    "import re                                          # Library untuk teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b501db0",
   "metadata": {},
   "source": [
    "**Scrapping Data Review Teks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76d3234b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Pasangan Calon</th>\n",
       "      <th>Text Tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>Banyak akun kloning seolah2 pendukung #agussil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>#agussilvy bicara apa kasihan yaa...lap itu ai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>Kalau aku sih gak nunggu hasil akhir QC tp lag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>Kasian oh kasian dengan peluru 1milyar untuk t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>Agus-Sylvi</td>\n",
       "      <td>Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id Sentiment Pasangan Calon  \\\n",
       "0   1  negative     Agus-Sylvi   \n",
       "1   2  negative     Agus-Sylvi   \n",
       "2   3  negative     Agus-Sylvi   \n",
       "3   4  negative     Agus-Sylvi   \n",
       "4   5  negative     Agus-Sylvi   \n",
       "\n",
       "                                          Text Tweet  \n",
       "0  Banyak akun kloning seolah2 pendukung #agussil...  \n",
       "1  #agussilvy bicara apa kasihan yaa...lap itu ai...  \n",
       "2  Kalau aku sih gak nunggu hasil akhir QC tp lag...  \n",
       "3  Kasian oh kasian dengan peluru 1milyar untuk t...  \n",
       "4  Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hasil_Scrapping = pd.read_csv('https://raw.githubusercontent.com/hfzhrahma/Kelas-Mahir/main/dataset_tweet_sentiment_pilkada_DKI_2017.csv')\n",
    "Hasil_Scrapping.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfda9cc",
   "metadata": {},
   "source": [
    "**Mengambil Series Data Teks Review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "284c1416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Banyak akun kloning seolah2 pendukung #agussil...\n",
       "1      #agussilvy bicara apa kasihan yaa...lap itu ai...\n",
       "2      Kalau aku sih gak nunggu hasil akhir QC tp lag...\n",
       "3      Kasian oh kasian dengan peluru 1milyar untuk t...\n",
       "4      Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...\n",
       "                             ...                        \n",
       "895    Kali saja bpk @aniesbaswedan @sandiuno lihat, ...\n",
       "896    Kita harus dapat merangkul semua orang tanpa b...\n",
       "897    Ini jagoanku dibidang digital <Smiling Face Wi...\n",
       "898                 #PesanBijak #OkeOce #GubernurGu3 ...\n",
       "899    Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...\n",
       "Name: Text Tweet, Length: 900, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teks = Hasil_Scrapping['Text Tweet']\n",
    "teks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52f4493",
   "metadata": {},
   "source": [
    "# Teks Preprocessing\n",
    "\n",
    "Setelah mendapat data teks, salah satu tantangan dari data adalah bentuknya yang sangat beragam. Sebuah kata dapat ditulis dengan berbagai bentuk. Kemudian juga besar sekali kemungkinan adalah kesalahan penulisan, tanda baca, angka, dan lain-lain. Oleh sebab itu, sebelum diolah lebih lanjut untuk diproses menjadi data numerik, maka diperlukan pemrosesan data teks agar menjadi bentuk yang lebih bersih dan standar. Yang akan  sangat mempengaruhi hasil analisis data teks tersebut. Pada sentimen analisis misalnya langkah ini menjadi sangat penting. Ada beberapa hal yang dilakukan pada tahap Teks Preprocessing:\n",
    "\n",
    "1.   **Case Folding dan Data Cleaning**\n",
    "\n",
    "Case folding adalah salah satu bentuk text preprocessing yang paling sederhana dan efektif meskipun sering diabaikan. Tujuan dari case folding untuk mengubah semua huruf dalam dokumen menjadi huruf kecil. Hanya huruf 'a' sampai 'z' yang diterima. Karakter selain huruf dihilangkan dan dianggap delimiter.\n",
    "\n",
    "Ada beberapa cara yang dapat digunakan dalam tahap folding, diantaranya:\n",
    "*   Menghapus tanda baca\n",
    "*   Menghapus angka\n",
    "*   Mengubah text menjadi lowercase\n",
    "*   Menghapus whitepace (karakter kosong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a1f0ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aku juga kemarin #AHY 19april2017 suaraku utk paslon 3 #DKI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'aku juga kemarin  AHY 19april2017 suaraku utk paslon 3  DKI'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menghapus tanda baca\n",
    "print(teks[13])\n",
    "teks[13]=re.sub(r'[^\\w]|_',' ',teks[13])\n",
    "teks[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "440fb7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ngawal Abang dulu keliling Ampenan #AHY #fkppi #kawal #bodyguard #military #myjob https://www.instagram.com/p/BTySqVcjLv5/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ngawal Abang dulu keliling Ampenan #AHY #fkppi #kawal #bodyguard #military #myjob'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menghapus angka\n",
    "print(teks[200])\n",
    "teks[200]=re.sub(\"\\S*\\d\\S*\", \"\", teks[200]).strip()\n",
    "teks[200]=re.sub(r\"\\b\\d+\\b\", \" \", teks[200])\n",
    "teks[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e602412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#AhokDjarot membuktikan bahwa tak selamanya mantan itu digantikan dengan yang lebih baik #mantanterindah #akakakakakk #maapnyampah\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#ahokdjarot membuktikan bahwa tak selamanya mantan itu digantikan dengan yang lebih baik #mantanterindah #akakakakakk #maapnyampah'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mengubah text menjadi lowercase\n",
    "print(teks[397])\n",
    "\n",
    "teks[397]=teks[397].lower()\n",
    "teks[397]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cea979f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AdjieRimbawan isu lama diangkat lagi, video lama dishare lagi, padahal video sudah disebar buzzer #AHY eh jelang #putaranke2 disebar lagi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'@AdjieRimbawan isu lama diangkat lagi, video lama dishare lagi, padahal video sudah disebar buzzer #AHY eh jelang #putaranke2 disebar lagi'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menghapus white space\n",
    "print(teks[28])\n",
    "\n",
    "teks[28]= re.sub('[\\s]+', ' ', teks[28])\n",
    "teks[28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bac43a",
   "metadata": {},
   "source": [
    "Membuat Fungsi untuk melakukan Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a370683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, unicodedata\n",
    "def Case_Folding(text):\n",
    "    # Hapus non-ascii\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', '')\n",
    "    \n",
    "    # Menghapus Tanda baca\n",
    "    text = re.sub(r'[^\\w]|_',' ', text)\n",
    "    \n",
    "    # Menghapus angka\n",
    "    text = re.sub(\"\\S*\\d\\S*\", \"\", text).strip()\n",
    "    text = re.sub(r\"\\b\\d+\\b\", \" \", text)\n",
    "    \n",
    "    # Mengubah text menjadi lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Menghapus white space\n",
    "    text = re.sub('[\\s]+', ' ', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374b30f7",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Proses pengurangan berbagai bentuk kata yang berubah menjadi satu bentuk untuk memudahkan analisis, e.g kata dari \"swim\", \"swimming\", \"swims\", \"swam\", adalah semua betuk dari \"swim\". Nah jadi lemma dari kata-kata tersebut adalah \"swim\".\n",
    "\n",
    "Untuk data teks berbahasa Indonesia, kita akan menggunakan library *nlp-id*. Pertama kita harus menginstallnya terlebih dahulu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "62a45e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nlp-id in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.1.12.0)\n",
      "Requirement already satisfied: scikit-learn==0.22 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nlp-id) (0.22)\n",
      "Requirement already satisfied: wget==3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nlp-id) (3.2)\n",
      "Requirement already satisfied: nltk==3.4.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nlp-id) (3.4.5)\n",
      "Requirement already satisfied: six in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from nltk==3.4.5->nlp-id) (1.15.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn==0.22->nlp-id) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn==0.22->nlp-id) (1.20.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn==0.22->nlp-id) (1.6.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install nlp-id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2620f80",
   "metadata": {},
   "source": [
    "Kemudian kita akan menggunakan fungsi *Lemmatizer()* untuk melakukan lemmatisasi data teks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61ab39fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sambutan hangat buat pendukung #AHY yang mau bergabung dengan kami <Smiling Face With Smiling Eyes> <FOLDED HANDS>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sambut hangat buat dukung ahy yang mau gabung dengan kami smiling face with smiling eyes folded hands'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlp_id.lemmatizer import Lemmatizer\n",
    "lemmatizer = Lemmatizer()\n",
    "print(teks[281])\n",
    "teks[281]=lemmatizer.lemmatize(teks[281])\n",
    "teks[281]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad37685",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "\n",
    "Stemming merupakan suatu proses untuk menemukan kata dasar dari sebuah kata. Dengan menghilangkan semua imbuhan (affixes) baik yang terdiri dari awalan (prefixes), sisipan (infixes), akhiran (suffixes) dan kombinasi dari awalan dan akhiran (cinfixes) pada kata turunan. Stemming digunakan untuk mengganti bentuk dari suatu kata menjadi kata dasar dari kata tersebut yang sesuai dengan struktur morfologi Bahasa Indonesia yang baik dan benar.\n",
    "\n",
    "Untuk data teks berbahasa Indonesia, kita akan menggunakan library *PySastrawi*. Pertama kita harus menginstallnya terlebih dahulu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0366481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PySastrawi in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PySastrawi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dc3503",
   "metadata": {},
   "source": [
    "Kemudian kita akan menggunakan fungsi *StemmerFactory()* untuk melakukan stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74c9d352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#ahy <Frowning Face>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ahy frowning face'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "\n",
    "# Membuat stemmer\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "print(teks[129])\n",
    "\n",
    "teks[129] = stemmer.stem(teks[129])\n",
    "teks[129]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bc9b14",
   "metadata": {},
   "source": [
    "## Slang Words\n",
    "\n",
    "Slang adalah kata-kata yang tidak baku secara bahasa namun sering dipakai oleh pengguna bahasa. Kita perlu melakukan standarisasi untuk slang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b6bface",
   "metadata": {},
   "outputs": [],
   "source": [
    "slang_dictionary = pd.read_csv('https://raw.githubusercontent.com/nikovs/data-science-portfolio/master/topic%20modelling/colloquial-indonesian-lexicon.csv')\n",
    "slang_dict = pd.Series(slang_dictionary['formal'].values, index=slang_dictionary['slang']).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3d62d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Slangwords(text):\n",
    "    for word in text.split():\n",
    "        if word in slang_dict.keys():\n",
    "            text = text.replace(word, slang_dict[word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09aa5e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lini pertahanan AHY rapuh debat efek, aksi 112 efek dan Antasari efek #pilkadaDKI\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'lini pertahanan AHY rapuh debat efek, aksi 112 efek dan Antasari efek #pilkadaDKI'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(teks[86])\n",
    "\n",
    "teks[86]=Slangwords(teks[86])\n",
    "teks[86]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205d4849",
   "metadata": {},
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words adalah kata umum (common words) yang biasanya mncul dalam jumlah besar dan dianggap tidak memiliki makna. Stop words umumnya dimanfaatkan dalam task information retrieval, termasuk oleh Google (penjelasannya di sini). Contoh stop words untuk bahasa Inggis diantaranya, \"of\", \"the\". Sedangkan untuk bahasa Indonesia diantaranya \"yang\", \"di\", \"ke\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c369e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlp_id.stopword import StopWord\n",
    "stopword = StopWord()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "689bbd56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heeebohhhh....Agus Sylvi kalah di TPS 6 tmpat @AgusYudhoyono menggunakan hak pilihnya... #PilkadaDKI #QuickCount #ahy\t\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'heeebohhhh.... Agus Sylvi kalah TPS 6 tmpat@AgusYudhoyono hak pilihnya...#PilkadaDKI#QuickCount#ahy'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nlp_id.stopword import StopWord\n",
    "print(teks[99])\n",
    "\n",
    "teks[99]=stopword.remove_stopword(teks[99])\n",
    "teks[99]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577da182",
   "metadata": {},
   "source": [
    "## Unwanted Words\n",
    "\n",
    "Unwanted words adalah kata-kata yang berada di luar beberpa hal di atas namun perlu kita hapus. Kita bisa mendefinisikan sendiri kata-kata atau karakter yang ingin kita hilangkan dari data teks yang kita peroleh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "50d59392",
   "metadata": {},
   "outputs": [],
   "source": [
    "unwanted_words = ['sy', 'karna', 'gue', 'pun', 'nya', 'yg', 'gw', 'ke', 'gak', \n",
    "                 'ga', 'buat', 'selama', 'akan', 'gua', 'gw', 'gue', 'banget', \n",
    "                 'mohon', 'dii', 'kalo', 'dll', 'cuman', 'cuma', 'biar', 'kayak', \n",
    "                 'ssaja', 'sih', 'si', 'situ', 'e', 'diin', 'serba', 'untuj', 'deh', \n",
    "                 'jd', 'ku', 'lg', 'and', 'tuh', 'nih', 'mas', 'mbak', 'tau', 'iya',\n",
    "                 'ya', 'lu', 'pas', 'wkwk', 'haha', 'wkwkwk', 'wkwkw', 'wow', 'akak',\n",
    "                 'anjir', 'lo', 'loh', 'bang', 'kak', 'jd', 'eh', 'oh', 'yuk', 'gila',\n",
    "                 'asa', 'mending', 'engenggak', 'a', 'mah', 'kali', 'piss', 'dlu', 'eh', 'ttp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d089eebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\LENOVO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def RemoveUnwantedWords(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence =[word for word in word_tokens if not word in unwanted_words]\n",
    "    return ' '.join(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad83f441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yuk #AHY Fans dukung dan menangkan #GubernurMuslim\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yuk # AHY Fans dukung dan menangkan # GubernurMuslim'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(teks[241])\n",
    "\n",
    "teks[241] = RemoveUnwantedWords(teks[241])\n",
    "teks[241]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9f4827",
   "metadata": {},
   "source": [
    "## Menerapkan Semua Langkah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6efd7992",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hasil_Scrapping['content_processed'] = ''\n",
    "\n",
    "for i, row in Hasil_Scrapping.iterrows():\n",
    "    text_tweet = Hasil_Scrapping['Text Tweet'][i]\n",
    "    result = Case_Folding(text_tweet)\n",
    "    result = lemmatizer.lemmatize(result)\n",
    "    result = stemmer.stem(result)\n",
    "    result = stopword.remove_stopword(result)\n",
    "    result = RemoveUnwantedWords(result)\n",
    "    Hasil_Scrapping['content_processed'][i]=result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7443182f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text Tweet</th>\n",
       "      <th>content_processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Banyak akun kloning seolah2 pendukung #agussil...</td>\n",
       "      <td>akun kloning dukung agussilvy serang paslon an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#agussilvy bicara apa kasihan yaa...lap itu ai...</td>\n",
       "      <td>agussilvy bicara kasihan yaa lap air mata wkwk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kalau aku sih gak nunggu hasil akhir QC tp lag...</td>\n",
       "      <td>nunggu hasil qc tp nunggu motif cuit sbyudhoyo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kasian oh kasian dengan peluru 1milyar untuk t...</td>\n",
       "      <td>kasi kasi peluru rw agussilvy mempan menangin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...</td>\n",
       "      <td>maaf dukung agussilvy hayo dukung aniessandi p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>Kali saja bpk @aniesbaswedan @sandiuno lihat, ...</td>\n",
       "      <td>bpk aniesbaswedan sandiuno lihat rspun selfie ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>Kita harus dapat merangkul semua orang tanpa b...</td>\n",
       "      <td>rangkul orang batas usia kelamin okeoce ok han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>897</th>\n",
       "      <td>Ini jagoanku dibidang digital &lt;Smiling Face Wi...</td>\n",
       "      <td>jago bidang digital smiling face with sunglass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>898</th>\n",
       "      <td>#PesanBijak #OkeOce #GubernurGu3 ...</td>\n",
       "      <td>pesanbijak okeoce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899</th>\n",
       "      <td>Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...</td>\n",
       "      <td>sandiaga bangun rumah dp simpel banding tol ci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Text Tweet  \\\n",
       "0    Banyak akun kloning seolah2 pendukung #agussil...   \n",
       "1    #agussilvy bicara apa kasihan yaa...lap itu ai...   \n",
       "2    Kalau aku sih gak nunggu hasil akhir QC tp lag...   \n",
       "3    Kasian oh kasian dengan peluru 1milyar untuk t...   \n",
       "4    Maaf ya pendukung #AgusSilvy..hayo dukung #Ani...   \n",
       "..                                                 ...   \n",
       "895  Kali saja bpk @aniesbaswedan @sandiuno lihat, ...   \n",
       "896  Kita harus dapat merangkul semua orang tanpa b...   \n",
       "897  Ini jagoanku dibidang digital <Smiling Face Wi...   \n",
       "898               #PesanBijak #OkeOce #GubernurGu3 ...   \n",
       "899  Sandiaga: Bangun Rumah DP 0% Lebih Simpel Diba...   \n",
       "\n",
       "                                     content_processed  \n",
       "0    akun kloning dukung agussilvy serang paslon an...  \n",
       "1    agussilvy bicara kasihan yaa lap air mata wkwk...  \n",
       "2    nunggu hasil qc tp nunggu motif cuit sbyudhoyo...  \n",
       "3    kasi kasi peluru rw agussilvy mempan menangin ...  \n",
       "4    maaf dukung agussilvy hayo dukung aniessandi p...  \n",
       "..                                                 ...  \n",
       "895  bpk aniesbaswedan sandiuno lihat rspun selfie ...  \n",
       "896  rangkul orang batas usia kelamin okeoce ok han...  \n",
       "897  jago bidang digital smiling face with sunglass...  \n",
       "898                                  pesanbijak okeoce  \n",
       "899  sandiaga bangun rumah dp simpel banding tol ci...  \n",
       "\n",
       "[900 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Hasil_Scrapping[['Text Tweet', 'content_processed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f07604fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hasil_Scrapping.to_csv('Pilkada_DKI.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2267b4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 900 entries, 0 to 899\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Id                 900 non-null    int64 \n",
      " 1   Sentiment          900 non-null    object\n",
      " 2   Pasangan Calon     900 non-null    object\n",
      " 3   Text Tweet         900 non-null    object\n",
      " 4   content_processed  900 non-null    object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 35.3+ KB\n"
     ]
    }
   ],
   "source": [
    "Hasil_Scrapping.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7904eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
